install.packages(c("ggplot2", "e1071","caret","quanteda",
                   "irlba","randomForest"))
spam.raw <- read.csv("spam.csv", stringsAsFactors = FALSE)
View(spam.raw)

spam.raw <- spam.raw[,1:2]
names(spam.raw) <- c("label", "text")

length(which(!complete.cases(spam.raw)))

spam.raw$label <- as.factor(spam.raw$label)

prop.table(table(spam.raw$label))

spam.raw$text_length <- nchar(spam.raw$text)
summary(spam.raw$text_length)

library(ggplot2)

ggplot(spam.raw, aes(x=text_length, fill=label)) + 
  theme_bw() +
  geom_histogram(binwidth=5) +
  labs(y="Text Count", x = "Length of text",
       title="Distribution of Text lengths with Class labels")

library(caret)
help(package="caret")

set.seed(32984)
indexes <- createDataPartition(spam.raw$label, times = 1,
                               p = 0.7, list = FALSE)

train <- spam.raw[indexes,]
test <- spam.raw[-indexes,]

prop.table(table(train$label))
prop.table(table(test$label))

train$text[21]

train$text[38]

train$text[357]

library("quanteda")
help(package="quanteda")

?tokens

train.tokens <- tokens(train$text, what="word",
                       remove_numbers = TRUE, 
                       remove_punct = TRUE,
                       remove_symbols = TRUE,
                       remove_hyphens = TRUE)

train.tokens[[357]]
train.tokens[38]

train.tokens <- tokens_tolower(train.tokens)
train.tokens[[357]]

train.tokens <- tokens_select(train.tokens, stopwords(),
                              selection = "remove")
train.tokens[[357]]

train.tokens <- tokens_wordstem(train.tokens, language = "english")
train.tokens[[357]]

train.tokens.dfm <- dfm(train.tokens, tolower=FALSE)

train.tokens.matrix <- as.matrix(train.tokens.dfm)
View(train.tokens.matrix[1:20, 1:100])
dim(train.tokens.matrix)

colnames(train.tokens.matrix)[1:50]

install.packages("tidyverse")
library(tidyverse)

train.tokens.dfm2 <- convert(train.tokens.dfm,to =c("data.frame"),docvars = NULL)
train.tokens.df <- cbind(Label = train$label, train.tokens.dfm2)

names(train.tokens.df) <- make.names(names(train.tokens.df))

names(train.tokens.df)[c(146, 148, 235, 238)]

library(caret)
set.seed(48743)
cv.folds <- createMultiFolds(train$label, k=10,times=3)

cv.cntrl <- trainControl(method="repeatedcv",number=10,
                         repeats=3,
                         index=cv.folds)

install.packages("doSNOW")
library(doSNOW)

start.time <- Sys.time()

cl <- makeCluster(1, type="SOCK")
registerDoSNOW(cl)


rpart.cv.1 <- train(Label ~., data=train.tokens.df,
                    method="rpart",
                    trControl=cv.cntrl,
                    tuneLength=7)


total.time <- Sys.time() - start.time
total.time
# time taken is 55 minutes.

rpart.cv.1
# cp=0.009560229
# accuracy=0.9515496
# kappa=0.7667952

term.frequency <- function(row){
  row/sum(row)
}

inverse.doc.freq <- function(col){
  corpus.size <- length(col)
  doc.count <- length(which(col>0))
  
  log10(corpus.size/doc.count)
}

tf.idf <- function(tf, idf){
  tf*idf
}

train.tokens.df <- apply(train.tokens.matrix,1,term.frequency)
dim(train.tokens.df)
View(train.tokens.df[1:20,1:100])

train.tokens.idf <- apply(train.tokens.matrix,2,inverse.doc.freq)
str(train.tokens.idf)

train.tokens.tfidf <- apply(train.tokens.df,2,tf.idf, idf=train.tokens.idf)
dim(train.tokens.tfidf)
View(train.tokens.tfidf[1:25,1:25])

train.tokens.tfidf <- t(train.tokens.tfidf)
dim(train.tokens.tfidf)
View(train.tokens.tfidf[1:25,1:25])

incomplete.cases <- which(!complete.cases(train.tokens.tfidf))
train$text[incomplete.cases]

train.tokens.tfidf[incomplete.cases,] <- rep(0.0,
                                             ncol(train.tokens.tfidf))
dim(train.tokens.tfidf)
sum(which(!complete.cases(train.tokens.idf)))

train.tokens.tfidf.df <- cbind(Label=train$label,
                               data.frame(train.tokens.tfidf))
names(train.tokens.tfidf.df) <- make.names(names(train.tokens.tfidf.df))
